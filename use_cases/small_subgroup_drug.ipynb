{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7413eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ctgan.synthesizers.ctgan import CTGANSynthesizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from src.metrics import *\n",
    "from src.metrics import eval_plugin\n",
    "from src.synthesizer import fit_ctgan\n",
    "from src.utils import *\n",
    "\n",
    "seed = 0\n",
    "seed_everything(seed)\n",
    "\n",
    "dataset_name = \"drug\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68da07c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import load_drug_dataset\n",
    "\n",
    "X, y, Data = load_drug_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_oracle = train_test_split(Data, test_size=0.5, random_state=seed)\n",
    "\n",
    "X_train, X_test = train_test_split(X_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "X_train, X_hp = train_test_split(X_train, test_size=0.1, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339e8fe",
   "metadata": {},
   "source": [
    "# Train Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ctgans = []\n",
    "trials = []\n",
    "params_list = []\n",
    "\n",
    "discrete_columns = [\n",
    "    \"Age\",\n",
    "    \"Gender\",\n",
    "    \"Education\",\n",
    "    \"Country\",\n",
    "    \"Ethnicity\",\n",
    "    \"Impulsive\",\n",
    "    \"SS\",\n",
    "    \"Alcohol\",\n",
    "    \"Amphet\",\n",
    "    \"Amyl\",\n",
    "    \"Benzos\",\n",
    "    \"Cannabis\",\n",
    "    \"Coke\",\n",
    "    \"Crack\",\n",
    "    \"Ecstasy\",\n",
    "    \"Heroin\",\n",
    "    \"Ketamine\",\n",
    "    \"Legalh\",\n",
    "    \"LSD\",\n",
    "    \"Meth\",\n",
    "    \"Mushrooms\",\n",
    "    \"VSA\",\n",
    "    \"y\",\n",
    "]\n",
    "\n",
    "use_trained_model = False\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "hp_sweep = False\n",
    "if use_trained_model == False:\n",
    "    if hp_sweep:\n",
    "        import optuna\n",
    "        from optuna.samplers import TPESampler\n",
    "\n",
    "        # Optimize HPS\n",
    "        def gan_objective(trial):\n",
    "\n",
    "            discrete_columns = [\n",
    "                \"Age\",\n",
    "                \"Gender\",\n",
    "                \"Education\",\n",
    "                \"Country\",\n",
    "                \"Ethnicity\",\n",
    "                \"Impulsive\",\n",
    "                \"SS\",\n",
    "                \"Alcohol\",\n",
    "                \"Amphet\",\n",
    "                \"Amyl\",\n",
    "                \"Benzos\",\n",
    "                \"Cannabis\",\n",
    "                \"Coke\",\n",
    "                \"Crack\",\n",
    "                \"Ecstasy\",\n",
    "                \"Heroin\",\n",
    "                \"Ketamine\",\n",
    "                \"Legalh\",\n",
    "                \"LSD\",\n",
    "                \"Meth\",\n",
    "                \"Mushrooms\",\n",
    "                \"VSA\",\n",
    "                \"y\",\n",
    "            ]\n",
    "\n",
    "            learning_rate = trial.suggest_categorical(\n",
    "                \"learning_rate\", [2e-4, 2e-5, 2e-6]\n",
    "            )\n",
    "            embedding_dim = trial.suggest_categorical(\"embedding_dim\", [64, 128, 256])\n",
    "            epochs = trial.suggest_categorical(\"epochs\", [200, 300, 500])\n",
    "\n",
    "            ctgan = fit_ctgan(\n",
    "                data=X_test,\n",
    "                epochs=epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                embedding_dim=embedding_dim,\n",
    "                discrete_columns=discrete_columns,\n",
    "            )\n",
    "            D_fake, _ = ctgan.sample(X_hp.shape[0], shift=False)\n",
    "            metric = MaximumMeanDiscrepancy\n",
    "            trial_results = eval_plugin(\n",
    "                metric,\n",
    "                GenericDataLoader(X_hp.astype(float)),\n",
    "                GenericDataLoader(D_fake.astype(float)),\n",
    "            )\n",
    "            trials.append(trial_results)\n",
    "            print(\n",
    "                eval_plugin(\n",
    "                    WassersteinDistance,\n",
    "                    GenericDataLoader(X_hp.astype(float)),\n",
    "                    GenericDataLoader(D_fake.astype(float)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print(f\"HPS = Params: {trial.params} | Score: {trial_results[0]}\")\n",
    "            return trial_results[0][\"joint\"]\n",
    "\n",
    "        gan_study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "        gan_study.optimize(gan_objective, show_progress_bar=True, n_trials=10)\n",
    "        print(\"Best parameters:\", gan_study.best_params)\n",
    "\n",
    "        ctgan = fit_ctgan(\n",
    "            data=X_test,\n",
    "            epochs=gan_study.best_params[\"epochs\"],\n",
    "            learning_rate=gan_study.best_params[\"learning_rate\"],\n",
    "            embedding_dim=gan_study.best_params[\"embedding_dim\"],\n",
    "            discrete_columns=discrete_columns,\n",
    "        )\n",
    "        ctgans.append(deepcopy(ctgan))\n",
    "\n",
    "    else:\n",
    "\n",
    "        from copy import deepcopy\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        for i in tqdm(range(10)):\n",
    "\n",
    "            best_params = {\"learning_rate\": 0.0002, \"embedding_dim\": 256, \"epochs\": 100}\n",
    "            ctgan = fit_ctgan(\n",
    "                data=X_train,\n",
    "                epochs=best_params[\"epochs\"],\n",
    "                learning_rate=best_params[\"learning_rate\"],\n",
    "                embedding_dim=best_params[\"embedding_dim\"],\n",
    "                seed=seed + i,\n",
    "                discrete_columns=discrete_columns,\n",
    "            )\n",
    "\n",
    "            D_fake, _ = ctgan.sample(X_hp.shape[0], shift=False)\n",
    "\n",
    "            trial_results = eval_plugin(\n",
    "                MaximumMeanDiscrepancy,\n",
    "                GenericDataLoader(X_hp.astype(float)),\n",
    "                GenericDataLoader(D_fake.astype(float)),\n",
    "            )\n",
    "            trials.append(trial_results)\n",
    "            params_list.append(best_params)\n",
    "            ctgans.append(deepcopy(ctgan))\n",
    "\n",
    "        # save each ctgan model in ctgan_list\n",
    "        for idx, ctgan_save in enumerate(ctgans):\n",
    "            ctgan_save.save(f\"../models/ctgan_{dataset_name}_{idx+1}\")\n",
    "\n",
    "        trials_list = [trials[idx][0][\"joint\"] for idx in range(len(trials))]\n",
    "        ctgan_idx = trials_list.index(min(trials_list))\n",
    "\n",
    "        ctgan = ctgans[ctgan_idx]\n",
    "\n",
    "    ctgan.save(f\"../models/ctgan_{dataset_name}\")\n",
    "\n",
    "    # pickle best params\n",
    "    with open(f\"../models/ctgan_{dataset_name}_params.pkl\", \"wb\") as f:\n",
    "        pickle.dump(params_list[ctgan_idx], f)\n",
    "\n",
    "\n",
    "else:\n",
    "    # Load best_params\n",
    "    with open(f\"../models/ctgan_{dataset_name}_params.pkl\", \"rb\") as f:\n",
    "        best_params = pickle.load(f)\n",
    "\n",
    "    ctgan = CTGANSynthesizer(\n",
    "        embedding_dim=best_params[\"embedding_dim\"],\n",
    "        generator_dim=(256, 256),\n",
    "        discriminator_dim=(256, 256),\n",
    "        generator_lr=best_params[\"learning_rate\"],\n",
    "        generator_decay=1e-6,\n",
    "        discriminator_lr=best_params[\"learning_rate\"],\n",
    "        discriminator_decay=1e-6,\n",
    "        batch_size=500,\n",
    "        discriminator_steps=1,\n",
    "        log_frequency=True,\n",
    "        verbose=False,\n",
    "        epochs=best_params[\"epochs\"],\n",
    "        pac=10,\n",
    "        cuda=True,\n",
    "    )\n",
    "\n",
    "    ctgan = ctgan.load(f\"../models/ctgan_{dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb75be",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from fairlearn.metrics import demographic_parity_ratio as dp_ratio\n",
    "from fairlearn.metrics import equalized_odds_ratio as eo_ratio\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_models(models, data, column_metric, sensitive=\"sex\"):\n",
    "    performance = {\"acc\": {}, \"f1\": {}, \"eo\": {}, \"dp\": {}}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        if data.shape[0] == 0:\n",
    "            performance[\"acc\"][model_name] = 0\n",
    "            performance[\"f1\"][model_name] = 0\n",
    "            performance[\"eo\"][model_name] = 0\n",
    "            performance[\"dp\"][model_name] = 0\n",
    "            continue\n",
    "\n",
    "        y_pred = model.predict(data.drop(\"y\", axis=1))\n",
    "        accuracy = accuracy_score(data[\"y\"], y_pred)\n",
    "        F1 = f1_score(data[\"y\"], y_pred)\n",
    "\n",
    "        try:\n",
    "            eo_score = eo_ratio(\n",
    "                data[\"y\"], y_pred, sensitive_features=data[sensitive].values\n",
    "            )\n",
    "        except:\n",
    "            eo_score = 0\n",
    "\n",
    "        try:\n",
    "            dp_score = dp_ratio(\n",
    "                data[\"y\"], y_pred, sensitive_features=data[sensitive].values\n",
    "            )\n",
    "        except:\n",
    "            dp_score = 0\n",
    "\n",
    "        performance[\"acc\"][model_name] = accuracy\n",
    "        performance[\"f1\"][model_name] = F1\n",
    "        performance[\"eo\"][model_name] = eo_score\n",
    "        performance[\"dp\"][model_name] = dp_score\n",
    "\n",
    "    return performance\n",
    "\n",
    "\n",
    "def run_analysis(\n",
    "    column_metric,\n",
    "    ctgan,\n",
    "    Data,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    X_oracle,\n",
    "    trained_model_dict,\n",
    "    random_state=0,\n",
    "    n_samples=1000,\n",
    "):\n",
    "    # Initialization\n",
    "    groups = list(np.unique(Data[column_metric]))\n",
    "    X_train, X_oracle = train_test_split(Data, test_size=0.65, random_state=0)\n",
    "    X_train, X_test = train_test_split(\n",
    "        X_train, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    test_dataset = deepcopy(X_test)\n",
    "\n",
    "    (\n",
    "        test_results,\n",
    "        test_samples,\n",
    "        oracle_results,\n",
    "        oracle_samples,\n",
    "        synth_results,\n",
    "        synth_samples,\n",
    "        aug_results,\n",
    "        aug_samples,\n",
    "    ) = ({}, {}, {}, {}, {}, {}, {}, {})\n",
    "\n",
    "    for group in tqdm(groups):\n",
    "        total_samples = (\n",
    "            X_test[X_test[column_metric] == 0].shape[0] if group == 0 else n_samples\n",
    "        )\n",
    "        test_data = X_test[X_test[column_metric] == group]\n",
    "        oracle_data = X_oracle[X_oracle[column_metric] == group]\n",
    "\n",
    "        # Dtest\n",
    "        test_results[group], test_samples[group] = (\n",
    "            evaluate_models(trained_model_dict, test_data, column_metric),\n",
    "            test_data.shape[0],\n",
    "        )\n",
    "        oracle_results[group], oracle_samples[group] = (\n",
    "            evaluate_models(trained_model_dict, oracle_data, column_metric),\n",
    "            oracle_data.shape[0],\n",
    "        )\n",
    "\n",
    "        # Synthetic data sampling\n",
    "        synth_data, _ = ctgan.sample(\n",
    "            1, shift=False, condition_column=column_metric, condition_value=group\n",
    "        )\n",
    "        count = 0\n",
    "        while synth_data.shape[0] <= total_samples:\n",
    "            tmp_df = ctgan.sample(\n",
    "                n_samples,\n",
    "                shift=False,\n",
    "                condition_column=column_metric,\n",
    "                condition_value=group,\n",
    "            )[0]\n",
    "            synth_data = synth_data.append(tmp_df[tmp_df[column_metric] == group])\n",
    "            count += 1\n",
    "        synth_results[group], synth_samples[group] = (\n",
    "            evaluate_models(trained_model_dict, synth_data, column_metric),\n",
    "            synth_data.shape[0],\n",
    "        )\n",
    "\n",
    "        # Augmented data evaluation\n",
    "        aug_data = pd.concat([test_data, synth_data])\n",
    "        aug_results[group], aug_samples[group] = (\n",
    "            evaluate_models(trained_model_dict, aug_data, column_metric),\n",
    "            aug_data.shape[0],\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        test_results,\n",
    "        test_samples,\n",
    "        oracle_results,\n",
    "        oracle_samples,\n",
    "        synth_results,\n",
    "        synth_samples,\n",
    "        aug_results,\n",
    "        aug_samples,\n",
    "        test_dataset,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27666251",
   "metadata": {},
   "source": [
    "# Train the downstream predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "column_metric = \"Ethnicity\"\n",
    "\n",
    "model_dict = {\n",
    "    \"mlp\": MLPClassifier(random_state=seed),\n",
    "    \"knn\": KNeighborsClassifier(),\n",
    "    \"dt\": DecisionTreeClassifier(random_state=seed),\n",
    "    \"rf\": RandomForestClassifier(random_state=seed),\n",
    "    \"gbc\": GradientBoostingClassifier(random_state=seed),\n",
    "    \"bag\": BaggingClassifier(random_state=seed),\n",
    "    \"ada\": AdaBoostClassifier(random_state=seed),\n",
    "    \"svm\": SVC(random_state=seed),\n",
    "    \"lr\": LogisticRegression(random_state=seed),\n",
    "}\n",
    "\n",
    "print(\"training baseline models\")\n",
    "trained_model_dict = train_models(X_train, model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_list = [trials[idx][0][\"joint\"] for idx in range(len(trials))]\n",
    "ctgan_idx = trials_list.index(min(trials_list))\n",
    "ctgan_idx=0\n",
    "ctgan = ctgans[ctgan_idx]\n",
    "ctgan_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd6aaa",
   "metadata": {},
   "source": [
    "# Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "oracle_acc_list = []\n",
    "oracle_samples_list = []\n",
    "test_acc_list = []\n",
    "test_samples_list = []\n",
    "synth_acc_list = []\n",
    "synth_samples_list = []\n",
    "aug_acc_list = []\n",
    "aug_samples_list = []\n",
    "final_test_dataset = []\n",
    "\n",
    "\n",
    "n_runs = 5\n",
    "ns = 2000\n",
    "\n",
    "for i in tqdm(range(n_runs)):\n",
    "\n",
    "    done = False\n",
    "    tries = 0\n",
    "\n",
    "    while done == False:\n",
    "        try:\n",
    "            (\n",
    "                test_accs,\n",
    "                test_samples,\n",
    "                oracle_accs,\n",
    "                oracle_samples,\n",
    "                synth_accs,\n",
    "                synth_samples,\n",
    "                aug_accs,\n",
    "                aug_samples,\n",
    "                test_dataset,\n",
    "            ) = run_analysis(\n",
    "                column_metric=column_metric,\n",
    "                Data=Data,\n",
    "                X_train=X_train,\n",
    "                X_test=X_test,\n",
    "                X_oracle=X_oracle,\n",
    "                ctgan=ctgan,\n",
    "                trained_model_dict=trained_model_dict,\n",
    "                random_state=i * 100,\n",
    "                n_samples=ns,\n",
    "            )\n",
    "            done = True\n",
    "            final_test_dataset.append(test_dataset)\n",
    "        except Exception:\n",
    "            import traceback\n",
    "\n",
    "            print(traceback.format_exc())\n",
    "            tries += 1\n",
    "\n",
    "            if tries > 1:\n",
    "                done = True\n",
    "            continue\n",
    "\n",
    "    test_acc_list.append(test_accs)\n",
    "    test_samples_list.append(test_samples)\n",
    "    oracle_acc_list.append(oracle_accs)\n",
    "    oracle_samples_list.append(oracle_samples)\n",
    "    synth_acc_list.append(synth_accs)\n",
    "    synth_samples_list.append(synth_samples)\n",
    "    aug_acc_list.append(aug_accs)\n",
    "    aug_samples_list.append(aug_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b7a38",
   "metadata": {},
   "source": [
    "# Process results & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brmp import brm\n",
    "from brmp.numpyro_backend import backend as numpyro\n",
    "from brmp.pyro_backend import backend as pyro_backend\n",
    "\n",
    "backend = pyro_backend\n",
    "\n",
    "metrics = [\"acc\"]\n",
    "models = list(trained_model_dict.keys())\n",
    "\n",
    "X_test_new = deepcopy(final_test_dataset[0])\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    for model in [\"rf\"]:\n",
    "\n",
    "        try:\n",
    "            mbm = []\n",
    "            for i in range(5):\n",
    "                seed = i * 100\n",
    "                X_train, X_oracle = train_test_split(\n",
    "                    Data, test_size=0.65, random_state=0\n",
    "                )\n",
    "                X_train, X_test = train_test_split(\n",
    "                    X_train, test_size=0.2, random_state=seed\n",
    "                )\n",
    "                X_test[\"S\"] = np.max(\n",
    "                    trained_model_dict[model].predict_proba(X_test.drop(\"y\", axis=1)),\n",
    "                    axis=1,\n",
    "                )\n",
    "                model3 = brm(\"S ~ Gender + Ethnicity + Education + Country + y\", X_test)\n",
    "                fit3 = model3.fit(backend=backend, seed=seed, iter=1000, warmup=100)\n",
    "                scores3 = fit3.fitted(what=\"sample\", data=None, seed=seed)\n",
    "                mbm.append(scores3)\n",
    "\n",
    "            mbm = np.array(mbm)\n",
    "\n",
    "            props = [\n",
    "                count / np.sum(np.unique(Data[column_metric], return_counts=True)[1])\n",
    "                for count in np.unique(Data[column_metric], return_counts=True)[1]\n",
    "            ]\n",
    "\n",
    "            groups = np.sort(list(X_oracle[column_metric].unique()))\n",
    "\n",
    "            for model_name in [model]:\n",
    "                print(model_name)\n",
    "                data_list = []\n",
    "                idx = 0\n",
    "\n",
    "                df = pd.DataFrame(columns=[\"Group\", \"3S\", \"3S+\", \"MBM\", \"Dtest\"])\n",
    "                df_std = pd.DataFrame(columns=[\"Group\", \"3S\", \"3S+\", \"MBM\", \"Dtest\"])\n",
    "\n",
    "                for group in np.argsort(props)[::-1]:\n",
    "                    if group == 2:\n",
    "                        group = 4\n",
    "                    idx += 1\n",
    "                    mylist = oracle_acc_list\n",
    "                    oracle_res = np.array(\n",
    "                        [\n",
    "                            mylist[i][group][metric][model_name]\n",
    "                            for i in range(len(mylist))\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    mylist = synth_acc_list\n",
    "                    synth_res = np.array(\n",
    "                        [\n",
    "                            mylist[i][group][metric][model_name]\n",
    "                            for i in range(len(mylist))\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    mylist = test_acc_list\n",
    "                    test_res = np.array(\n",
    "                        [\n",
    "                            mylist[i][group][metric][model_name]\n",
    "                            for i in range(len(mylist))\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    mylist = aug_acc_list\n",
    "                    aug_res = np.array(\n",
    "                        [\n",
    "                            mylist[i][group][metric][model_name]\n",
    "                            for i in range(len(mylist))\n",
    "                        ]\n",
    "                    )\n",
    "                    try:\n",
    "                        # mbm\n",
    "                        blm_res = []\n",
    "                        for i in range(5):\n",
    "                            y_pred = (np.mean(mbm[i], axis=0) > 0.75).astype(int) # only confident predictions\n",
    "                            group_ids = np.argwhere(\n",
    "                                np.array(X_test_new[column_metric] == group).astype(int)\n",
    "                                == 1\n",
    "                            )\n",
    "                            y_pred_group = y_pred[group_ids]\n",
    "                            y_true_group = X_test_new[\n",
    "                                X_test_new[column_metric] == group\n",
    "                            ][\"y\"]\n",
    "                            blm_res.append(accuracy_score(y_true_group, y_pred_group))\n",
    "\n",
    "                        blm_res = np.array(blm_res)\n",
    "\n",
    "                        mydict = {\n",
    "                        \"Group\": f\"{idx} ({int(round(props[group]*100,0))}%)\",\n",
    "                        \"3S\": round(np.mean(np.abs(oracle_res - synth_res)) * 100, 2),\n",
    "                        \"3S+\": round(np.mean(np.abs(oracle_res - aug_res)) * 100, 2),\n",
    "                        \"MBM\": round(np.mean(np.abs(oracle_res - blm_res)) * 100, 2),\n",
    "                        \"Dtest\": round(np.mean(np.abs(oracle_res - test_res)) * 100, 2),\n",
    "                        }\n",
    "                        df = df.append(mydict, ignore_index=True)\n",
    "\n",
    "                        mydict = {\n",
    "                            \"Group\": f\"{idx} ({int(round(props[group]*100,0))}%)\",\n",
    "                            \"3S\": round(np.std(np.abs(oracle_res - synth_res)) * 100, 2),\n",
    "                            \"3S+\": round(np.std(np.abs(oracle_res - aug_res)) * 100, 2),\n",
    "                            \"MBM\": round(np.std(np.abs(oracle_res - blm_res)) * 100, 2),\n",
    "                            \"Dtest\": round(np.std(np.abs(oracle_res - test_res)) * 100, 2),\n",
    "                        }\n",
    "\n",
    "\n",
    "                        df_std = df_std.append(mydict, ignore_index=True)\n",
    "\n",
    "                    except Exception:\n",
    "                        import traceback\n",
    "\n",
    "                        print(traceback.format_exc())\n",
    "                        continue\n",
    "\n",
    "            df.to_csv(f\"../results/{dataset_name}_{model}_{metric}.csv\")\n",
    "            df_std.to_csv(f'../results/{dataset_name}_std_{model}_{metric}.csv')\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "\n",
    "            print(traceback.format_exc())\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d090201",
   "metadata": {},
   "source": [
    "# Coverage helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "from brmp import brm\n",
    "from brmp.numpyro_backend import backend as numpyro\n",
    "from brmp.pyro_backend import backend as pyro_backend\n",
    "\n",
    "\n",
    "def get_group(X_test, column_metric, groups, test_group=\"small\"):\n",
    "    smallest_group = 0\n",
    "    gsizes = []\n",
    "    for idx, group in enumerate(groups):\n",
    "        gsizes.append(X_test[X_test[column_metric] == group].shape[0])\n",
    "\n",
    "    print(gsizes)\n",
    "    ns = 1\n",
    "    for idx, group in enumerate(groups):\n",
    "\n",
    "        if test_group == \"small\":\n",
    "            if idx == 0:\n",
    "                size_smallest_group = X_test[X_test[column_metric] == 0].shape[0]\n",
    "                smallest_group = group\n",
    "                if size_smallest_group < ns:\n",
    "                    size_smallest_group = 99999\n",
    "            else:\n",
    "                if (\n",
    "                    X_test[X_test[column_metric] == group].shape[0]\n",
    "                    < size_smallest_group\n",
    "                ):\n",
    "                    if X_test[X_test[column_metric] == group].shape[0] > ns:\n",
    "                        size_smallest_group = X_test[\n",
    "                            X_test[column_metric] == group\n",
    "                        ].shape[0]\n",
    "                        smallest_group = group\n",
    "\n",
    "        else:\n",
    "            if idx == 0:\n",
    "                size_smallest_group = X_test[X_test[column_metric] == 0].shape[0]\n",
    "                smallest_group = group\n",
    "            else:\n",
    "                if (\n",
    "                    X_test[X_test[column_metric] == group].shape[0]\n",
    "                    > size_smallest_group\n",
    "                ):\n",
    "                    size_smallest_group = X_test[X_test[column_metric] == group].shape[\n",
    "                        0\n",
    "                    ]\n",
    "                    smallest_group = group\n",
    "\n",
    "    return smallest_group\n",
    "\n",
    "\n",
    "def uncertainty(\n",
    "    column_metric,\n",
    "    ctgans,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    X_oracle,\n",
    "    trained_model_dict,\n",
    "    random_state=0,\n",
    "    n_samples=5000,\n",
    "    test_group=\"small\",\n",
    "):\n",
    "    from sklearn.metrics import f1_score\n",
    "    from fairlearn.metrics import equalized_odds_ratio as eo_ratio\n",
    "    from fairlearn.metrics import demographic_parity_ratio as dp_ratio\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    score = \"acc\"\n",
    "\n",
    "    groups = list(np.unique(Data[column_metric]))\n",
    "    orig_random_state = deepcopy(random_state)\n",
    "\n",
    "    coverage_dict = {}\n",
    "    width_dict = {}\n",
    "    excess_dict = {}\n",
    "    deficet_dict = {}\n",
    "\n",
    "    for model in [\"rf\"]:  \n",
    "\n",
    "        random_state = orig_random_state\n",
    "\n",
    "  \n",
    "        try:\n",
    "            trained_model_dict[model].predict_proba(X_train.drop(\"y\", axis=1))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        test_coverage = []\n",
    "        synth_coverage = []\n",
    "        synth_coverage5 = []\n",
    "        synth_coverage10 = []\n",
    "        blm_coverage = []\n",
    "\n",
    "        test_width = []\n",
    "        synth_width = []\n",
    "        synth_width5 = []\n",
    "        synth_width10 = []\n",
    "        blm_width = []\n",
    "\n",
    "        test_excess = []\n",
    "        synth_excess = []\n",
    "        synth_excess5 = []\n",
    "        synth_excess10 = []\n",
    "        blm_excess = []\n",
    "\n",
    "        test_deficet = []\n",
    "        synth_deficet = []\n",
    "        synth_deficet5 = []\n",
    "        synth_deficet10 = []\n",
    "        blm_deficet = []\n",
    "\n",
    "        error_test = []\n",
    "        error_synth = []\n",
    "\n",
    "        for random_state in tqdm(range(20)):\n",
    "\n",
    "            _, X_test = train_test_split(\n",
    "                X_train, test_size=0.2, random_state=random_state\n",
    "            )\n",
    "\n",
    "            deepcopy(X_test)\n",
    "\n",
    "        \n",
    "            blm_accs = {}\n",
    "\n",
    "       \n",
    "            smallest_group = get_group(\n",
    "                X_test, column_metric, groups, test_group=\"small\"\n",
    "            )\n",
    "            print(f\"Smallest group: {smallest_group}, random_state: {random_state}\")\n",
    "\n",
    "            for group in groups:\n",
    "\n",
    "                if group != smallest_group:\n",
    "                    continue\n",
    "\n",
    "                # n_samples = 1000\n",
    "                if group == 0:\n",
    "                    total_samples = X_test[X_test[column_metric] == 0].shape[0]\n",
    "                else:\n",
    "                    total_samples = n_samples\n",
    "\n",
    "                test_data = X_test[X_test[column_metric] == group]\n",
    "\n",
    "                oracle_data = X_oracle[X_oracle[column_metric] == group]\n",
    "\n",
    "                print(f\"TEST SIZE {group} - {test_data.shape[0]}\")\n",
    "\n",
    "                if test_data.shape[0] == 0:\n",
    "                    accuracy = 0\n",
    "\n",
    "                else:\n",
    "                    slack = 0.025  # slack for the confidence interval\n",
    "                    clf = model_dict[model]\n",
    "\n",
    "                    ############################################################\n",
    "                    # Oracle\n",
    "                    ############################################################\n",
    "                    y_pred = clf.predict(oracle_data.drop(\"y\", axis=1))\n",
    "                    accuracy_oracle = accuracy_score(oracle_data[\"y\"], y_pred)\n",
    "                    print(oracle_data.shape, accuracy_oracle)\n",
    "\n",
    "                    ############################################################\n",
    "                    # Bootstrap Dtest\n",
    "                    ############################################################\n",
    "                    y_pred = clf.predict(test_data.drop(\"y\", axis=1))\n",
    "                    accuracy = accuracy_score(test_data[\"y\"], y_pred)\n",
    "                    R = 1000\n",
    "                    confidence_level = 0.95\n",
    "                    metric = accuracy_score\n",
    "                    scores = bootstrap(\n",
    "                        test_data[\"y\"].values.reshape(-1, 1),\n",
    "                        y_pred.reshape(-1, 1),\n",
    "                        metric,\n",
    "                        R,\n",
    "                    )\n",
    "                    bottom_test, top_test = confidence_intervals(\n",
    "                        scores, confidence_level\n",
    "                    )\n",
    "                    score = 0\n",
    "                    if (\n",
    "                        accuracy_oracle >= bottom_test.values - slack\n",
    "                        and accuracy_oracle <= top_test.values + slack\n",
    "                    ):\n",
    "                        score = 1\n",
    "                    test_coverage.append(score)\n",
    "                    error_test.append(accuracy_oracle - accuracy)\n",
    "                    excess, deficet, width = compute_interval_metrics(\n",
    "                        lb=bottom_test.values - slack,\n",
    "                        ub=top_test.values + slack,\n",
    "                        true=accuracy_oracle,\n",
    "                    )\n",
    "                    test_width.append(width)\n",
    "                    if excess != -9999:\n",
    "                        test_excess.append(excess)\n",
    "                    if deficet != -9999:\n",
    "                        test_deficet.append(deficet)\n",
    "\n",
    "                    ############################################################\n",
    "                    # MBM Interval\n",
    "                    ############################################################\n",
    "                    X_test_new = deepcopy(X_test)\n",
    "                    X_test_new[\"S\"] = trained_model_dict[model].predict_proba(\n",
    "                        X_test_new.drop(\"y\", axis=1)\n",
    "                    )[:, 1]\n",
    "\n",
    "                    model3 = brm(\n",
    "                        \"S ~ Gender + Ethnicity + Education + Country + y\", X_test_new\n",
    "                    )\n",
    "                    fit3 = model3.fit(\n",
    "                        backend=pyro_backend, seed=0, iter=1000, warmup=100\n",
    "                    )\n",
    "                    myscores = fit3.fitted(what=\"sample\", data=None, seed=0)\n",
    "\n",
    "                    blm_accs = []\n",
    "                    for j in range(100):\n",
    "                        scores3 = np.array(\n",
    "                            [\n",
    "                                myscores[np.random.randint(1000, size=1), i]\n",
    "                                for i in range(myscores.shape[1])\n",
    "                            ]\n",
    "                        )\n",
    "                        y_pred = (scores3 > 0.75).astype(int).flatten()\n",
    "                        group_ids = np.argwhere(\n",
    "                            np.array(X_test_new[column_metric] == group).astype(int)\n",
    "                            == 1\n",
    "                        )\n",
    "                        y_pred_group = y_pred[group_ids]\n",
    "                        acc = accuracy_score(test_data[\"y\"], y_pred_group)\n",
    "                        blm_accs.append(acc)\n",
    "\n",
    "                    mean_blm_acc = np.mean(blm_accs)\n",
    "                    std_blm_acc = np.std(blm_accs)\n",
    "                    bottom_blm = mean_blm_acc - 1.96 * std_blm_acc\n",
    "                    top_blm = mean_blm_acc + 1.96 * std_blm_acc\n",
    "\n",
    "                    score = 0\n",
    "                    if (\n",
    "                        accuracy_oracle >= bottom_blm - slack\n",
    "                        and accuracy_oracle <= top_blm + slack\n",
    "                    ):\n",
    "                        score = 1\n",
    "                    blm_coverage.append(score)\n",
    "                    excess, deficet, width = compute_interval_metrics(\n",
    "                        lb=bottom_blm - slack, ub=top_blm + slack, true=accuracy_oracle\n",
    "                    )\n",
    "                    blm_width.append(width)\n",
    "                    if excess != -9999:\n",
    "                        blm_excess.append(excess)\n",
    "                    if deficet != -9999:\n",
    "                        blm_deficet.append(deficet)\n",
    "\n",
    "                    ############################################################\n",
    "                    # 3S (k=1)\n",
    "                    ############################################################\n",
    "                    syn_accs = []\n",
    "                    for i in range(100):\n",
    "                        shift_df, _ = ctgans[0].sample(\n",
    "                            1,\n",
    "                            shift=False,\n",
    "                            condition_column=column_metric,\n",
    "                            condition_value=group,\n",
    "                        )\n",
    "                        count = 0\n",
    "                        while shift_df.shape[0] <= total_samples:\n",
    "                            generated_tmp, _ = ctgans[0].sample(\n",
    "                                n_samples,\n",
    "                                shift=False,\n",
    "                                condition_column=column_metric,\n",
    "                                condition_value=group,\n",
    "                            )\n",
    "                            tmp_df = generated_tmp[\n",
    "                                generated_tmp[column_metric] == group\n",
    "                            ]\n",
    "                            shift_df = shift_df.append(tmp_df)\n",
    "                            count += 1\n",
    "\n",
    "                        syn_data = shift_df[shift_df[column_metric] == group]\n",
    "                        y_pred = clf.predict(syn_data.drop(\"y\", axis=1))\n",
    "                        accuracy = accuracy_score(syn_data[\"y\"], y_pred)\n",
    "                        syn_accs.append(accuracy)\n",
    "\n",
    "                    mean_syn_acc = np.mean(syn_accs)\n",
    "                    std_syn_acc = np.std(syn_accs)\n",
    "                    bottom_syn = mean_syn_acc - 1.96 * std_syn_acc\n",
    "                    top_syn = mean_syn_acc + 1.96 * std_syn_acc\n",
    "\n",
    "                    ############################################################\n",
    "                    # 3S (k=5)\n",
    "                    ############################################################\n",
    "                    syn_accs5 = []\n",
    "                    for ctgan in ctgans[0:5]:\n",
    "                        shift_df, _ = ctgan.sample(\n",
    "                            1,\n",
    "                            shift=False,\n",
    "                            condition_column=column_metric,\n",
    "                            condition_value=group,\n",
    "                        )\n",
    "\n",
    "                        count = 0\n",
    "                        while shift_df.shape[0] <= total_samples:\n",
    "                            generated_tmp, _ = ctgan.sample(\n",
    "                                n_samples,\n",
    "                                shift=False,\n",
    "                                condition_column=column_metric,\n",
    "                                condition_value=group,\n",
    "                            )\n",
    "                            tmp_df = generated_tmp[\n",
    "                                generated_tmp[column_metric] == group\n",
    "                            ]\n",
    "                            shift_df = shift_df.append(tmp_df)\n",
    "                            count += 1\n",
    "\n",
    "                        syn_data = shift_df[shift_df[column_metric] == group]\n",
    "                        y_pred = clf.predict(syn_data.drop(\"y\", axis=1))\n",
    "                        accuracy = accuracy_score(syn_data[\"y\"], y_pred)\n",
    "                        syn_accs5.append(accuracy)\n",
    "\n",
    "                    mean_syn_acc = np.mean(syn_accs5)\n",
    "                    std_syn_acc = np.std(syn_accs5)\n",
    "                    bottom_syn5 = mean_syn_acc - 1.96 * std_syn_acc\n",
    "                    top_syn5 = mean_syn_acc + 1.96 * std_syn_acc\n",
    "\n",
    "                    ############################################################\n",
    "                    # 3S (k=10)\n",
    "                    ############################################################\n",
    "                    syn_accs10 = []\n",
    "                    for ctgan in ctgans:\n",
    "                        shift_df, _ = ctgan.sample(\n",
    "                            1,\n",
    "                            shift=False,\n",
    "                            condition_column=column_metric,\n",
    "                            condition_value=group,\n",
    "                        )\n",
    "\n",
    "                        count = 0\n",
    "                        while shift_df.shape[0] <= total_samples:\n",
    "                            generated_tmp, _ = ctgan.sample(\n",
    "                                n_samples,\n",
    "                                shift=False,\n",
    "                                condition_column=column_metric,\n",
    "                                condition_value=group,\n",
    "                            )\n",
    "                            tmp_df = generated_tmp[\n",
    "                                generated_tmp[column_metric] == group\n",
    "                            ]\n",
    "                            shift_df = shift_df.append(tmp_df)\n",
    "                            count += 1\n",
    "\n",
    "                        syn_data = shift_df[shift_df[column_metric] == group]\n",
    "                        y_pred = clf.predict(syn_data.drop(\"y\", axis=1))\n",
    "                        accuracy = accuracy_score(syn_data[\"y\"], y_pred)\n",
    "                        syn_accs10.append(accuracy)\n",
    "\n",
    "                    mean_syn_acc = np.mean(syn_accs10)\n",
    "                    std_syn_acc = np.std(syn_accs10)\n",
    "                    error_synth.append(accuracy_oracle - mean_syn_acc)\n",
    "                    bottom_syn10 = mean_syn_acc - 1.96 * std_syn_acc\n",
    "                    top_syn10 = mean_syn_acc + 1.96 * std_syn_acc\n",
    "\n",
    "                    # compute if the accuracy_oracle is within the confidence interval of the accuracy\n",
    "                    score = 0\n",
    "                    if (\n",
    "                        accuracy_oracle >= bottom_syn - slack\n",
    "                        and accuracy_oracle <= top_syn + slack\n",
    "                    ):\n",
    "                        score = 1\n",
    "\n",
    "                    synth_coverage.append(score)\n",
    "                    excess, deficet, width = compute_interval_metrics(\n",
    "                        lb=bottom_syn - slack, ub=top_syn + slack, true=accuracy_oracle\n",
    "                    )\n",
    "                    synth_width.append(width)\n",
    "                    if excess != -9999:\n",
    "                        synth_excess.append(excess)\n",
    "                    if deficet != -9999:\n",
    "                        synth_deficet.append(deficet)\n",
    "\n",
    "                    score = 0\n",
    "                    if (\n",
    "                        accuracy_oracle >= bottom_syn5 - slack\n",
    "                        and accuracy_oracle <= top_syn5 + slack\n",
    "                    ):\n",
    "                        score = 1\n",
    "                    synth_coverage5.append(score)\n",
    "                    excess, deficet, width = compute_interval_metrics(\n",
    "                        lb=bottom_syn5 - slack,\n",
    "                        ub=top_syn5 + slack,\n",
    "                        true=accuracy_oracle,\n",
    "                    )\n",
    "                    synth_width5.append(width)\n",
    "                    if excess != -9999:\n",
    "                        synth_excess5.append(excess)\n",
    "                    if deficet != -9999:\n",
    "                        synth_deficet5.append(deficet)\n",
    "\n",
    "                    score = 0\n",
    "                    if (\n",
    "                        accuracy_oracle >= bottom_syn10 - slack\n",
    "                        and accuracy_oracle <= top_syn10 + slack\n",
    "                    ):\n",
    "                        score = 1\n",
    "                    synth_coverage10.append(score)\n",
    "                    excess, deficet, width = compute_interval_metrics(\n",
    "                        lb=bottom_syn10 - slack,\n",
    "                        ub=top_syn10 + slack,\n",
    "                        true=accuracy_oracle,\n",
    "                    )\n",
    "                    synth_width10.append(width)\n",
    "                    if excess != -9999:\n",
    "                        synth_excess10.append(excess)\n",
    "                    if deficet != -9999:\n",
    "                        synth_deficet10.append(deficet)\n",
    "\n",
    "        mean_test_coverage = np.mean(test_coverage)\n",
    "        mean_blm_coverage = np.mean(blm_coverage)\n",
    "        mean_synth_coverage = np.mean(synth_coverage)\n",
    "        mean_synth_coverage5 = np.mean(synth_coverage5)\n",
    "        mean_synth_coverage10 = np.mean(synth_coverage10)\n",
    "        np.mean(error_test)\n",
    "        np.mean(error_synth)\n",
    "\n",
    "        # mean for width for synth, synth2, synth3, test, mbm\n",
    "        mean_synth_width = np.mean(synth_width)\n",
    "        mean_synth_width5 = np.mean(synth_width5)\n",
    "        mean_synth_width10 = np.mean(synth_width10)\n",
    "        mean_test_width = np.mean(test_width)\n",
    "        mean_blm_width = np.mean(blm_width)\n",
    "\n",
    "        # excess for width for synth, synth2, synth3, test, mbm\n",
    "        mean_synth_excess = np.mean(synth_excess)\n",
    "        mean_synth_excess5 = np.mean(synth_excess5)\n",
    "        mean_synth_excess10 = np.mean(synth_excess10)\n",
    "        mean_test_excess = np.mean(test_excess)\n",
    "        mean_blm_excess = np.mean(blm_excess)\n",
    "\n",
    "        # mean for excess for synth, synth2, synth3, test, mbm\n",
    "        mean_synth_deficet = np.mean(synth_deficet)\n",
    "        mean_synth_deficet5 = np.mean(synth_deficet5)\n",
    "        mean_synth_deficet10 = np.mean(synth_deficet10)\n",
    "        mean_test_deficet = np.mean(test_deficet)\n",
    "        mean_blm_deficet = np.mean(blm_deficet)\n",
    "\n",
    "        # coverage dict\n",
    "        coverage_dict[model] = {\n",
    "            \"test\": mean_test_coverage,\n",
    "            \"synth\": mean_synth_coverage,\n",
    "            \"synth2\": mean_synth_coverage5,\n",
    "            \"synth3\": mean_synth_coverage10,\n",
    "            \"MBM\": mean_blm_coverage,\n",
    "        }\n",
    "\n",
    "        # width dict\n",
    "        width_dict[model] = {\n",
    "            \"test\": mean_test_width,\n",
    "            \"synth\": mean_synth_width,\n",
    "            \"synth2\": mean_synth_width5,\n",
    "            \"synth3\": mean_synth_width10,\n",
    "            \"MBM\": mean_blm_width,\n",
    "        }\n",
    "\n",
    "        # excess dict\n",
    "        excess_dict[model] = {\n",
    "            \"test\": mean_test_excess,\n",
    "            \"synth\": mean_synth_excess,\n",
    "            \"synth2\": mean_synth_excess5,\n",
    "            \"synth3\": mean_synth_excess10,\n",
    "            \"MBM\": mean_blm_excess,\n",
    "        }\n",
    "\n",
    "        # deficet dict\n",
    "        deficet_dict[model] = {\n",
    "            \"test\": mean_test_deficet,\n",
    "            \"synth\": mean_synth_deficet,\n",
    "            \"synth2\": mean_synth_deficet5,\n",
    "            \"synth3\": mean_synth_deficet10,\n",
    "            \"MBM\": mean_blm_deficet,\n",
    "        }\n",
    "\n",
    "    return coverage_dict, width_dict, excess_dict, deficet_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6a10e",
   "metadata": {},
   "source": [
    "# Run coverage uncertainty exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    coverage, width, excess, deficet = uncertainty(\n",
    "        column_metric=column_metric,\n",
    "        ctgans=ctgans[0:10],\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        X_oracle=X_oracle,\n",
    "        trained_model_dict=trained_model_dict,\n",
    "        random_state=i * 100,\n",
    "        test_group=\"small\",\n",
    "    )\n",
    "    done = True\n",
    "\n",
    "except Exception:\n",
    "    import traceback\n",
    "\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36105fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = \"drug\"\n",
    "folder = \"../results\"\n",
    "for model in list(coverage.keys()):\n",
    "    mdf_coverage = pd.DataFrame.from_dict(\n",
    "        coverage[model], orient=\"index\", columns=[\"Value\"]\n",
    "    )\n",
    "    mdf_coverage.to_csv(f\"{folder}/coverage_{dname}_{model}.csv\")\n",
    "\n",
    "for model in list(width.keys()):\n",
    "    mdf_width = pd.DataFrame.from_dict(width[model], orient=\"index\", columns=[\"Value\"])\n",
    "    mdf_width.to_csv(f\"{folder}/width_{dname}_{model}.csv\")\n",
    "\n",
    "for model in list(excess.keys()):\n",
    "    mdf_excess = pd.DataFrame.from_dict(\n",
    "        excess[model], orient=\"index\", columns=[\"Value\"]\n",
    "    )\n",
    "    mdf_excess.to_csv(f\"{folder}/excess_{dname}_{model}.csv\")\n",
    "\n",
    "for model in list(deficet.keys()):\n",
    "    mdf_deficit = pd.DataFrame.from_dict(\n",
    "        deficet[model], orient=\"index\", columns=[\"Value\"]\n",
    "    )\n",
    "    mdf_deficit.to_csv(f\"{folder}/deficet_{dname}_{model}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3s_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
